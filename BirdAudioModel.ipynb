{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n! pip install pandarallel\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input/birdclef-2025/train_audio'):\n    # for filename in filenames:\n        # print(os.path.join(dirname, filename))\n        # pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"06865f8c-efd2-44a8-9292-7e9f22e0885a","_cell_guid":"c0492972-a264-4978-981e-127588cc3709","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:17:28.798237Z","iopub.execute_input":"2025-05-04T10:17:28.798999Z","iopub.status.idle":"2025-05-04T10:17:38.578448Z","shell.execute_reply.started":"2025-05-04T10:17:28.798959Z","shell.execute_reply":"2025-05-04T10:17:38.577114Z"}},"outputs":[{"name":"stdout","text":"Collecting pandarallel\n  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from pandarallel) (0.3.8)\nRequirement already satisfied: pandas>=1 in /usr/local/lib/python3.10/dist-packages (from pandarallel) (2.2.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from pandarallel) (5.9.5)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1->pandarallel) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1->pandarallel) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1->pandarallel) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1->pandarallel) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1->pandarallel) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1->pandarallel) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1->pandarallel) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas>=1->pandarallel) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas>=1->pandarallel) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas>=1->pandarallel) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas>=1->pandarallel) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas>=1->pandarallel) (2024.2.0)\nBuilding wheels for collected packages: pandarallel\n  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16674 sha256=3ad493822c13a9349a2391b66c899b52cc8dbfb26d552da150ddf5d5a0759e69\n  Stored in directory: /root/.cache/pip/wheels/50/4f/1e/34e057bb868842209f1623f195b74fd7eda229308a7352d47f\nSuccessfully built pandarallel\nInstalling collected packages: pandarallel\nSuccessfully installed pandarallel-1.6.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"labels = {'brtpar1', '1139490', 'compau', 'chbant1', 'yehcar1', 'yecspi2', 'watjac1', 'grasal4', 'grbhaw1', \n          'yebfly1', 'neocor', '81930', 'spbwoo1', '64862', 'grepot1', 'ruther1', 'banana', 'whttro1', \n          '1462711', '42087', '66531', 'soulap1', 'amakin1', '41970', '65373', '714022', 'bafibi1', 'blcant4', \n          'rutjac1', 'plbwoo1', 'anhing', 'yehbla2', '21211', 'recwoo1', 'blbgra1', 'creoro1', 'shtfly1', \n          'amekes', 'blchaw1', '21116', '566513', 'bugtan', 'strcuc1', '1564122', '1462737', 'purgal2', \n          'socfly1', 'gohman1', 'gycwor1', 'bubwre1', 'blhpar1', '65336', 'solsan', '134933', '24292', \n          '42113', 'plukit1', 'savhaw1', 'sobtyr1', 'chfmac1', 'yebsee1', '66016', 'blbwre1', 'mastit1', \n          'smbani', 'whfant1', 'strfly1', 'roahaw', 'rumfly1', '476537', 'butsal1', 'bucmot3', 'colcha1', \n          'bobfly1', '67082', 'rebbla1', 'pavpig2', '1192948', 'whbman1', 'verfly', 'eardov1', 'norscr1', \n          'rinkin1', '67252', 'greibi1', 'greegr', 'cattyr', 'laufal1', 'trokin', 'grekis', 'crebob1', \n          'bubcur1', 'fotfly', 'palhor2', '476538', '24322', 'tropar', 'whwswa1', 'yercac1', '517119', \n          '24272', 'cocher1', 'labter1', 'bicwre1', 'compot1', 'olipic1', 'blcjay1', 'colara1', 'spepar1', \n          'cregua1', 'cargra1', '22976', 'plctan1', '715170', 'leagre', '22973', 'bkmtou1', 'yelori1', \n          'trsowl', 'strher', 'ragmac1', 'yeofly1', '548639', 'tbsfin1', '135045', '65344', 'bkcdon', \n          'stbwoo2', 'piepuf1', '868458', '963335', 'blctit1', 'saffin', 'rtlhum', 'royfly1', '66893', \n          'rutpuf1', 'linwoo1', 'wbwwre1', 'srwswa1', '126247', 'gretin1', 'grnkin', 'littin1', 'secfly1', \n          '41778', '528041', 'bbwduc', 'greani1', 'rubsee1', 'orcpar', 'rosspo1', 'yebela1', '47067', \n          'crcwoo1', '65349', 'snoegr', 'gybmar', 'thbeup1', '66578', 'turvul', 'rugdov', 'baymac', \n          'speowl1', 'cocwoo1', 'cotfly1', 'y00678', '65419', 'bobher1', '52884', '41663', '22333', \n          'piwtyr1', '21038', '787625', 'rufmot1', '65962', 'paltan1', '48124', '555142', '65547', \n          'crbtan1', '1194042', 'ywcpar', 'shghum1', 'cinbec1', 'thlsch3', '1346504', '555086', 'sahpar1', \n          'grysee1', 'blkvul', '523060', 'strowl1', 'whbant1', 'whmtyr1', '65448', 'ampkin1', 'whtdov', \n          'yectyr1', '42007', '46010', 'pirfly1', 'woosto', 'babwar', '50186'}\nprint(len(labels))","metadata":{"_uuid":"7d1582c7-b934-4308-bef1-2cb926448b35","_cell_guid":"d12fcec0-c322-427d-94c5-a35fb6545838","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:17:38.580042Z","iopub.execute_input":"2025-05-04T10:17:38.580563Z","iopub.status.idle":"2025-05-04T10:17:38.593901Z","shell.execute_reply.started":"2025-05-04T10:17:38.580527Z","shell.execute_reply":"2025-05-04T10:17:38.592506Z"}},"outputs":[{"name":"stdout","text":"206\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom IPython.display import Audio\nfrom scipy.io import wavfile\nimport soundfile as sf\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.metrics import classification_report\nimport scipy.signal\nfrom tqdm import tqdm\nflag = 0","metadata":{"_uuid":"f2cccdd0-2b59-4596-8461-962c18bb52b7","_cell_guid":"7e2e0e2d-61f6-4f04-926a-6d3213cde37c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:17:38.595708Z","iopub.execute_input":"2025-05-04T10:17:38.596073Z","iopub.status.idle":"2025-05-04T10:17:58.426881Z","shell.execute_reply.started":"2025-05-04T10:17:38.596042Z","shell.execute_reply":"2025-05-04T10:17:58.425655Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def ensure_sample_rate(original_sample_rate, waveform, desired_sample_rate=22000):\n    if original_sample_rate != desired_sample_rate:\n        desired_length = int(\n            round(float(len(waveform))/original_sample_rate * desired_sample_rate))\n        waveform = scipy.signal.resample(waveform, desired_length)\n    return desired_sample_rate, waveform","metadata":{"_uuid":"50f45f5a-29ad-4b38-842e-763412cd51ea","_cell_guid":"6bfaabde-4d94-47b0-b0d5-ca75ba8e961e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:17:58.428336Z","iopub.execute_input":"2025-05-04T10:17:58.428985Z","iopub.status.idle":"2025-05-04T10:17:58.434345Z","shell.execute_reply.started":"2025-05-04T10:17:58.428952Z","shell.execute_reply":"2025-05-04T10:17:58.432847Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def read_audio(filename):\n    wav_data, sample_rate = sf.read(file=filename, dtype=np.int16)\n    if len(wav_data.shape) > 1:\n        wav_data = np.mean(wav_data, axis=1)\n    sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n    return sample_rate, wav_data","metadata":{"_uuid":"815b5ee9-300a-46c7-a58f-b34ef3e1b410","_cell_guid":"864fe43d-6dc2-4b7f-9aaf-eae3ca7506c0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:17:58.435487Z","iopub.execute_input":"2025-05-04T10:17:58.435836Z","iopub.status.idle":"2025-05-04T10:17:58.517883Z","shell.execute_reply.started":"2025-05-04T10:17:58.435805Z","shell.execute_reply":"2025-05-04T10:17:58.516810Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define the root directory containing the audio files\nroot_dir = '/kaggle/input/birdclef-2025/train_audio' # Adjust if your path is different\n\n# List to store the data for the DataFrame\naudio_data_list = []\n\n# Walk through the directory structure\nfor dirname, _, filenames in os.walk(root_dir):\n    # Skip the root directory itself if it doesn't contain class folders directly\n    # (Adjust this condition if your structure is different)\n    if dirname == root_dir:\n        continue\n\n    # Extract the class name (subdirectory name)\n    # os.path.basename gets the last part of the directory path\n    class_name = os.path.basename(dirname)\n\n    # Iterate through files in the current directory\n    for filename in filenames:\n        # Construct the full path to the audio file\n        full_path = os.path.join(dirname, filename)\n\n        # Append the file path and its class to the list\n        audio_data_list.append([full_path, class_name])\n        # You can remove the print statement if you don't need it anymore\n        # print(full_path) # Optional: print the path as it's processed\n\n# Create the Pandas DataFrame\naudio_dataframe = pd.DataFrame(audio_data_list, columns=[\"audio_path\", \"class\"])\n\n# Display the first few rows of the DataFrame (optional)\nprint(audio_dataframe.head())\n\n# Display the shape of the DataFrame (optional)\nprint(f\"\\nDataFrame shape: {audio_dataframe.shape}\")","metadata":{"_uuid":"4121086d-c342-4272-89c9-d3dc8774759d","_cell_guid":"1dca9713-a909-4e49-b6b0-82330ce3d220","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:17:58.519065Z","iopub.execute_input":"2025-05-04T10:17:58.519346Z","iopub.status.idle":"2025-05-04T10:18:26.636315Z","shell.execute_reply.started":"2025-05-04T10:17:58.519322Z","shell.execute_reply":"2025-05-04T10:18:26.635316Z"}},"outputs":[{"name":"stdout","text":"                                          audio_path    class\n0  /kaggle/input/birdclef-2025/train_audio/greani...  greani1\n1  /kaggle/input/birdclef-2025/train_audio/greani...  greani1\n2  /kaggle/input/birdclef-2025/train_audio/greani...  greani1\n3  /kaggle/input/birdclef-2025/train_audio/greani...  greani1\n4  /kaggle/input/birdclef-2025/train_audio/greani...  greani1\n\nDataFrame shape: (28564, 2)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df = audio_dataframe.iloc[0]\n\nfor i in audio_dataframe['class'].unique():\n    df_sub = audio_dataframe[audio_dataframe['class'] == i]\n    num_rows = 2\n    # print(len(df_sub.index))\n    if len(df_sub.index) < num_rows:\n        num_rows = len(df_sub.index)\n    # print(num_rows)\n    df_sub = audio_dataframe[audio_dataframe['class'] == i].iloc[:num_rows]\n    # print(df_sub)\n    df = pd.concat([df,df_sub])\ndf['class']\n\n# flag = 0","metadata":{"_uuid":"e22d37fd-03e9-47dd-85d6-0df6a5124199","_cell_guid":"e34eed76-cb7b-440e-9833-13cee54e7eaa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:18:26.637506Z","iopub.execute_input":"2025-05-04T10:18:26.637993Z","iopub.status.idle":"2025-05-04T10:18:27.977163Z","shell.execute_reply.started":"2025-05-04T10:18:26.637931Z","shell.execute_reply":"2025-05-04T10:18:27.976046Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"audio_path        NaN\nclass             NaN\n0             greani1\n1             greani1\n127           thbeup1\n               ...   \n27762          528041\n27763         yercac1\n27764         yercac1\n28065         wbwwre1\n28066         wbwwre1\nName: class, Length: 414, dtype: object"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport concurrent.futures\nfrom tqdm.auto import tqdm\nimport time # Just for mock function\nimport os # To get cpu count\nif flag == 0:\n    \n    # Get the list of paths to process\n    paths_to_process = df['audio_path'].tolist()\n\n    # Determine number of workers (e.g., number of CPU cores)\n    # Adjust based on memory constraints and task type (CPU vs I/O bound)\n    num_workers = os.cpu_count()\n    print(f\"Using {num_workers} workers.\")\n\n    results = [None] * len(paths_to_process) # Preallocate results list\n\n    print(\"Starting parallel processing with concurrent.futures...\")\n    # Use ProcessPoolExecutor for CPU-bound tasks\n    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n        # Use executor.map to apply the function in parallel\n        # Wrap executor.map with tqdm for a progress bar\n        # executor.map preserves the order of the input iterable\n        future_to_path = {executor.submit(read_audio, path): i for i, path in enumerate(paths_to_process)}\n\n        for future in tqdm(concurrent.futures.as_completed(future_to_path), total=len(paths_to_process), desc=\"Reading audio files\"):\n            index = future_to_path[future]\n            try:\n                results[index] = future.result()\n            except Exception as exc:\n                print(f'Path at index {index} generated an exception: {exc}')\n                results[index] = None # Or some other error indicator\n    flag = 1\n    print(\"Finished parallel processing.\")\n\n    # Assign the results back to the DataFrame\n    df['audio_data'] = results\n    df.to_pickle('/kaggle/working/df.pk1')\n    print(df.head())","metadata":{"_uuid":"a8b30ee5-27f6-4929-8939-116228480d8e","_cell_guid":"977fe82e-9a6e-4262-858d-9bdf56e4bd1a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:18:27.980063Z","iopub.execute_input":"2025-05-04T10:18:27.980385Z","iopub.status.idle":"2025-05-04T10:21:13.438730Z","shell.execute_reply.started":"2025-05-04T10:18:27.980360Z","shell.execute_reply":"2025-05-04T10:21:13.436822Z"}},"outputs":[{"name":"stdout","text":"Using 4 workers.\nStarting parallel processing with concurrent.futures...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Reading audio files:   0%|          | 0/414 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0fc3607cc224a7e8518f96e739ebb4c"}},"metadata":{}},{"name":"stdout","text":"Path at index 1 generated an exception: Invalid file: nan\nPath at index 0 generated an exception: Invalid file: nan\nFinished parallel processing.\n                                                            0  \\\naudio_path  /kaggle/input/birdclef-2025/train_audio/greani...   \nclass                                                 greani1   \n0                                                         NaN   \n1                                                         NaN   \n127                                                       NaN   \n\n                                                   audio_path    class  \\\naudio_path                                                NaN      NaN   \nclass                                                     NaN      NaN   \n0           /kaggle/input/birdclef-2025/train_audio/greani...  greani1   \n1           /kaggle/input/birdclef-2025/train_audio/greani...  greani1   \n127         /kaggle/input/birdclef-2025/train_audio/thbeup...  thbeup1   \n\n                                                   audio_data  \naudio_path                                               None  \nclass                                                    None  \n0           (22000, [0.37219046485309565, 0.08794960037434...  \n1           (22000, [-110.65109836872698, 31.9560972163999...  \n127         (22000, [0.02443199830545253, 0.15852787986383...  \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import re\n\ndef audio_data_unfucker(fucked_tuple: tuple) -> list:\n        assert(type(fucked_tuple[1]) != int)\n        return fucked_tuple[1]","metadata":{"_uuid":"c4191425-c246-40c9-9374-e5ba95ff31ef","_cell_guid":"a0870745-efe2-43a5-84ee-eb84e58b272d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:21:13.442096Z","iopub.execute_input":"2025-05-04T10:21:13.442716Z","iopub.status.idle":"2025-05-04T10:21:13.450817Z","shell.execute_reply.started":"2025-05-04T10:21:13.442624Z","shell.execute_reply":"2025-05-04T10:21:13.448844Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if flag == 1:\n    pass\nelse:\n    flag = 1\ndf = pd.read_pickle('/kaggle/working/df.pk1')\ndf = df.iloc[2:]\nprint(df.columns)\ndf['audio_data'] = df['audio_data'].apply(audio_data_unfucker)","metadata":{"_uuid":"e5124c66-5846-45c9-b0de-c024b252b875","_cell_guid":"4d65d6a6-763d-400d-b9ed-ba0683348fda","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:21:13.452825Z","iopub.execute_input":"2025-05-04T10:21:13.453358Z","iopub.status.idle":"2025-05-04T10:21:16.168882Z","shell.execute_reply.started":"2025-05-04T10:21:13.453309Z","shell.execute_reply":"2025-05-04T10:21:16.167446Z"}},"outputs":[{"name":"stdout","text":"Index([0, 'audio_path', 'class', 'audio_data'], dtype='object')\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"df['audio_data']","metadata":{"_uuid":"5858ca67-8a01-4f2a-b6ac-a69d9418105a","_cell_guid":"4596ff4f-2490-45db-ab98-d1d2d0bdbf5b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:21:16.170086Z","iopub.execute_input":"2025-05-04T10:21:16.170532Z","iopub.status.idle":"2025-05-04T10:21:16.188774Z","shell.execute_reply.started":"2025-05-04T10:21:16.170486Z","shell.execute_reply":"2025-05-04T10:21:16.187203Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0        [0.37219046485309565, 0.08794960037434989, -0....\n1        [-110.65109836872698, 31.956097216399932, -16....\n127      [0.02443199830545253, 0.15852787986383432, -0....\n128      [-160.59586225753768, 108.45083653120524, 69.3...\n280      [-1.1748742006981951, 0.4424288258146739, 0.21...\n                               ...                        \n27762    [-1.0507843878934087, -0.33029084259555325, 1....\n27763    [1.4450126382572601, -3.183779706907055, 2.787...\n27764    [0.045319001187525305, -0.43545604174867325, -...\n28065    [2.6276395042924694, -2.547362930612095, 2.518...\n28066    [0.23633340212851134, -0.05339789995365927, -0...\nName: audio_data, Length: 412, dtype: object"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"audio_data = df['audio_data'].copy()","metadata":{"_uuid":"3a81fb7f-2eed-4a4d-a30d-989cb47f221d","_cell_guid":"a645c135-06d9-4af0-abf6-31e6fdf726f5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:21:16.190782Z","iopub.execute_input":"2025-05-04T10:21:16.191241Z","iopub.status.idle":"2025-05-04T10:21:18.437476Z","shell.execute_reply.started":"2025-05-04T10:21:16.191195Z","shell.execute_reply":"2025-05-04T10:21:18.436019Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"max_len = 100  # Your desired fixed length\nprocessed_audio = []\nprocessed_labels = []\n\nprint(f\"Processing audio data with max_len = {max_len}\")\nprint(f\"Original DataFrame has {len(df)} rows.\")\n\n# Iterate through the DataFrame rows using itertuples (generally faster)\n# 'Index' is the DataFrame index, 'audio_data' and 'label' match column names\nfor row in tqdm(df.itertuples(), total=len(df), desc=\"Processing Audio\"):\n    # --- Get data for the current row ---\n    # Use getattr for robustness if column names might vary slightly\n    # print(row)\n    original_seq = getattr(row, 'audio_data')\n    original_label = getattr(row, '_3') # Fetch the label\n\n    # Ensure the sequence is a numpy array with the desired dtype\n    # This handles cases where 'audio_data' might contain lists\n    seq = np.array(original_seq, dtype=np.float32)\n    current_len = len(seq)\n\n    # --- Case 1: Sequence is shorter than max_len ---\n    if current_len < max_len:\n        padding_needed = max_len - current_len\n        # Use np.pad which can be cleaner for padding\n        padded_seq = np.pad(seq, (0, padding_needed), mode='constant', constant_values=0)\n        processed_audio.append(padded_seq)\n        processed_labels.append(original_label) # Append the original label\n\n    # --- Case 2: Sequence is exactly max_len ---\n    elif current_len == max_len:\n        processed_audio.append(seq) # No padding or truncation needed\n        processed_labels.append(original_label) # Append the original label\n\n    # --- Case 3: Sequence is longer than max_len ---\n    else:\n        # Calculate how many full chunks we can get\n        num_full_chunks = current_len // max_len\n\n        # Iterate through the sequence, extracting non-overlapping chunks\n        for i in range(num_full_chunks):\n            start_index = i * max_len\n            end_index = start_index + max_len\n            chunk = seq[start_index:end_index]\n            processed_audio.append(chunk)\n            # Append the SAME original label for EACH chunk\n            processed_labels.append(original_label)\n\n        # --- Handle the remainder (the part left over after full chunks) ---\n        remainder_len = current_len % max_len\n        if remainder_len > 0:\n            # Extract the remainder\n            remainder_start_index = num_full_chunks * max_len\n            remainder_chunk = seq[remainder_start_index:]\n\n            # Pad the remainder to max_len\n            padding_needed = max_len - remainder_len\n            padded_remainder = np.pad(remainder_chunk, (0, padding_needed), mode='constant', constant_values=0)\n\n            processed_audio.append(padded_remainder)\n            # Append the SAME original label for the padded remainder chunk\n            processed_labels.append(original_label)\n\n\n# --- Final Output ---\n# Convert lists to numpy arrays (common practice for ML/DL)\nfinal_audio_array = np.array(processed_audio)\n# Labels can be kept as a list or converted to numpy array/pandas Series\nfinal_labels = processed_labels # Or np.array(processed_labels)\n\nprint(f\"\\nFinished processing.\")\nprint(f\"Resulting audio array shape: {final_audio_array.shape}\")\n# Based on example: (1+1+2+1+1) = 6 samples -> (6, 100)\nprint(f\"Number of resulting labels: {len(final_labels)}\")\n# Based on example: 6 labels -> ['Short', 'Exact', 'Long', 'Long', 'Long', 'Short_2']\n# print(\"Example processed labels:\", final_labels)","metadata":{"_uuid":"532a3805-4956-4a69-8092-9cc4ecb70f22","_cell_guid":"88fd1701-e577-4f4d-819b-40827b85c463","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:21:18.438813Z","iopub.execute_input":"2025-05-04T10:21:18.439266Z","iopub.status.idle":"2025-05-04T10:21:40.514066Z","shell.execute_reply.started":"2025-05-04T10:21:18.439224Z","shell.execute_reply":"2025-05-04T10:21:40.511773Z"}},"outputs":[{"name":"stdout","text":"Processing audio data with max_len = 100\nOriginal DataFrame has 412 rows.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Audio:   0%|          | 0/412 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ec5b7d1cc84d459c7a20a2d853223f"}},"metadata":{}},{"name":"stdout","text":"\nFinished processing.\nResulting audio array shape: (3607422, 100)\nNumber of resulting labels: 3607422\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\nprint(f\"Original audio array shape: {final_audio_array.shape}\")\nprint(f\"Original number of labels: {len(final_labels)}\")\n\n\n### VARIABLE TO CHANGE\nlimit_per_label = 1000\n\n# 1. Create a DataFrame with labels and original indices\n#    This avoids putting the large audio array into the DataFrame, saving memory.\nprint(\"\\nCreating temporary DataFrame with labels and indices...\")\nindices_df = pd.DataFrame({\n    'label': final_labels,\n    'original_index': np.arange(len(final_labels)) # Store 0, 1, 2, ... N-1\n})\n\n# 2. Group by label and select the first 'limit_per_label' indices for each group\nprint(f\"Grouping by label and selecting up to {limit_per_label} indices per group...\")\n# .head(n) conveniently takes min(n, group_size) automatically\nselected_indices_df = indices_df.groupby('label', observed=True).head(limit_per_label)\n# 'observed=True' can speed up grouping if labels are categorical\n\n# --- Alternative: Random Sampling (if you don't want the *first* 100) ---\n# def sample_or_head(group, n):\n#     group_size = len(group)\n#     # Sample if group is large enough, otherwise take all (head)\n#     return group.sample(n=min(group_size, n), random_state=42) # Use random_state for reproducibility\n#\n# selected_indices_df = indices_df.groupby('label', observed=True).apply(sample_or_head, n=limit_per_label).reset_index(drop=True)\n# print(f\"Grouping by label and randomly sampling up to {limit_per_label} indices per group...\")\n# --- End Alternative ---\n\n\n# 3. Get the selected original indices\nselected_indices = selected_indices_df['original_index'].values\n\n# Ensure the indices are sorted if you want the final array order to be somewhat grouped by label\n# This is optional, .head() preserves original relative order within groups.\n# selected_indices = np.sort(selected_indices)\n\nprint(f\"Total indices selected: {len(selected_indices)}\")\n\n# 4. Use the selected indices to filter your original arrays\nprint(\"Filtering original audio array and labels using selected indices...\")\nlimited_audio_array = final_audio_array[selected_indices]\n\n# Ensure final_labels is a numpy array if it isn't already for fancy indexing\nfinal_labels_array = np.array(final_labels)\nlimited_labels_array = final_labels_array[selected_indices]\n\n# --- Verification (Optional) ---\nprint(\"\\n--- Verification ---\")\nprint(f\"Limited audio array shape: {limited_audio_array.shape}\")\nprint(f\"Limited labels array length: {len(limited_labels_array)}\")\n\n# Check counts per label in the limited set\nunique_labels, counts = np.unique(limited_labels_array, return_counts=True)\nprint(\"Counts per label in the limited dataset:\")\nfor label, count in zip(unique_labels, counts):\n    print(f\"  Label '{label}': {count} samples\")\n    if count > limit_per_label:\n        print(f\"  WARNING: Label '{label}' has more than {limit_per_label} samples!\") # Should not happen with .head()\n\nprint(\"\\nDownsampling complete.\")\n\n# Now use 'limited_audio_array' and 'limited_labels_array' for further steps","metadata":{"_uuid":"992bd950-31c2-419f-b60e-7bedf893bf28","_cell_guid":"a2f4c61b-20fe-438a-9c86-e120ac966b44","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:21:40.516418Z","iopub.execute_input":"2025-05-04T10:21:40.517041Z","iopub.status.idle":"2025-05-04T10:21:42.379066Z","shell.execute_reply.started":"2025-05-04T10:21:40.516969Z","shell.execute_reply":"2025-05-04T10:21:42.377808Z"}},"outputs":[{"name":"stdout","text":"Original audio array shape: (3607422, 100)\nOriginal number of labels: 3607422\n\nCreating temporary DataFrame with labels and indices...\nGrouping by label and selecting up to 1000 indices per group...\nTotal indices selected: 205604\nFiltering original audio array and labels using selected indices...\n\n--- Verification ---\nLimited audio array shape: (205604, 100)\nLimited labels array length: 205604\nCounts per label in the limited dataset:\n  Label '1139490': 1000 samples\n  Label '1192948': 1000 samples\n  Label '1194042': 1000 samples\n  Label '126247': 1000 samples\n  Label '1346504': 1000 samples\n  Label '134933': 1000 samples\n  Label '135045': 1000 samples\n  Label '1462711': 1000 samples\n  Label '1462737': 1000 samples\n  Label '1564122': 708 samples\n  Label '21038': 1000 samples\n  Label '21116': 1000 samples\n  Label '21211': 1000 samples\n  Label '22333': 1000 samples\n  Label '22973': 1000 samples\n  Label '22976': 1000 samples\n  Label '24272': 1000 samples\n  Label '24292': 1000 samples\n  Label '24322': 1000 samples\n  Label '41663': 1000 samples\n  Label '41778': 1000 samples\n  Label '41970': 1000 samples\n  Label '42007': 1000 samples\n  Label '42087': 1000 samples\n  Label '42113': 1000 samples\n  Label '46010': 1000 samples\n  Label '47067': 1000 samples\n  Label '476537': 1000 samples\n  Label '476538': 1000 samples\n  Label '48124': 1000 samples\n  Label '50186': 1000 samples\n  Label '517119': 1000 samples\n  Label '523060': 1000 samples\n  Label '528041': 1000 samples\n  Label '52884': 1000 samples\n  Label '548639': 896 samples\n  Label '555086': 1000 samples\n  Label '555142': 1000 samples\n  Label '566513': 1000 samples\n  Label '64862': 1000 samples\n  Label '65336': 1000 samples\n  Label '65344': 1000 samples\n  Label '65349': 1000 samples\n  Label '65373': 1000 samples\n  Label '65419': 1000 samples\n  Label '65448': 1000 samples\n  Label '65547': 1000 samples\n  Label '65962': 1000 samples\n  Label '66016': 1000 samples\n  Label '66531': 1000 samples\n  Label '66578': 1000 samples\n  Label '66893': 1000 samples\n  Label '67082': 1000 samples\n  Label '67252': 1000 samples\n  Label '714022': 1000 samples\n  Label '715170': 1000 samples\n  Label '787625': 1000 samples\n  Label '81930': 1000 samples\n  Label '868458': 1000 samples\n  Label '963335': 1000 samples\n  Label 'amakin1': 1000 samples\n  Label 'amekes': 1000 samples\n  Label 'ampkin1': 1000 samples\n  Label 'anhing': 1000 samples\n  Label 'babwar': 1000 samples\n  Label 'bafibi1': 1000 samples\n  Label 'banana': 1000 samples\n  Label 'baymac': 1000 samples\n  Label 'bbwduc': 1000 samples\n  Label 'bicwre1': 1000 samples\n  Label 'bkcdon': 1000 samples\n  Label 'bkmtou1': 1000 samples\n  Label 'blbgra1': 1000 samples\n  Label 'blbwre1': 1000 samples\n  Label 'blcant4': 1000 samples\n  Label 'blchaw1': 1000 samples\n  Label 'blcjay1': 1000 samples\n  Label 'blctit1': 1000 samples\n  Label 'blhpar1': 1000 samples\n  Label 'blkvul': 1000 samples\n  Label 'bobfly1': 1000 samples\n  Label 'bobher1': 1000 samples\n  Label 'brtpar1': 1000 samples\n  Label 'bubcur1': 1000 samples\n  Label 'bubwre1': 1000 samples\n  Label 'bucmot3': 1000 samples\n  Label 'bugtan': 1000 samples\n  Label 'butsal1': 1000 samples\n  Label 'cargra1': 1000 samples\n  Label 'cattyr': 1000 samples\n  Label 'chbant1': 1000 samples\n  Label 'chfmac1': 1000 samples\n  Label 'cinbec1': 1000 samples\n  Label 'cocher1': 1000 samples\n  Label 'cocwoo1': 1000 samples\n  Label 'colara1': 1000 samples\n  Label 'colcha1': 1000 samples\n  Label 'compau': 1000 samples\n  Label 'compot1': 1000 samples\n  Label 'cotfly1': 1000 samples\n  Label 'crbtan1': 1000 samples\n  Label 'crcwoo1': 1000 samples\n  Label 'crebob1': 1000 samples\n  Label 'cregua1': 1000 samples\n  Label 'creoro1': 1000 samples\n  Label 'eardov1': 1000 samples\n  Label 'fotfly': 1000 samples\n  Label 'gohman1': 1000 samples\n  Label 'grasal4': 1000 samples\n  Label 'grbhaw1': 1000 samples\n  Label 'greani1': 1000 samples\n  Label 'greegr': 1000 samples\n  Label 'greibi1': 1000 samples\n  Label 'grekis': 1000 samples\n  Label 'grepot1': 1000 samples\n  Label 'gretin1': 1000 samples\n  Label 'grnkin': 1000 samples\n  Label 'grysee1': 1000 samples\n  Label 'gybmar': 1000 samples\n  Label 'gycwor1': 1000 samples\n  Label 'labter1': 1000 samples\n  Label 'laufal1': 1000 samples\n  Label 'leagre': 1000 samples\n  Label 'linwoo1': 1000 samples\n  Label 'littin1': 1000 samples\n  Label 'mastit1': 1000 samples\n  Label 'neocor': 1000 samples\n  Label 'norscr1': 1000 samples\n  Label 'olipic1': 1000 samples\n  Label 'orcpar': 1000 samples\n  Label 'palhor2': 1000 samples\n  Label 'paltan1': 1000 samples\n  Label 'pavpig2': 1000 samples\n  Label 'piepuf1': 1000 samples\n  Label 'pirfly1': 1000 samples\n  Label 'piwtyr1': 1000 samples\n  Label 'plbwoo1': 1000 samples\n  Label 'plctan1': 1000 samples\n  Label 'plukit1': 1000 samples\n  Label 'purgal2': 1000 samples\n  Label 'ragmac1': 1000 samples\n  Label 'rebbla1': 1000 samples\n  Label 'recwoo1': 1000 samples\n  Label 'rinkin1': 1000 samples\n  Label 'roahaw': 1000 samples\n  Label 'rosspo1': 1000 samples\n  Label 'royfly1': 1000 samples\n  Label 'rtlhum': 1000 samples\n  Label 'rubsee1': 1000 samples\n  Label 'rufmot1': 1000 samples\n  Label 'rugdov': 1000 samples\n  Label 'rumfly1': 1000 samples\n  Label 'ruther1': 1000 samples\n  Label 'rutjac1': 1000 samples\n  Label 'rutpuf1': 1000 samples\n  Label 'saffin': 1000 samples\n  Label 'sahpar1': 1000 samples\n  Label 'savhaw1': 1000 samples\n  Label 'secfly1': 1000 samples\n  Label 'shghum1': 1000 samples\n  Label 'shtfly1': 1000 samples\n  Label 'smbani': 1000 samples\n  Label 'snoegr': 1000 samples\n  Label 'sobtyr1': 1000 samples\n  Label 'socfly1': 1000 samples\n  Label 'solsan': 1000 samples\n  Label 'soulap1': 1000 samples\n  Label 'spbwoo1': 1000 samples\n  Label 'speowl1': 1000 samples\n  Label 'spepar1': 1000 samples\n  Label 'srwswa1': 1000 samples\n  Label 'stbwoo2': 1000 samples\n  Label 'strcuc1': 1000 samples\n  Label 'strfly1': 1000 samples\n  Label 'strher': 1000 samples\n  Label 'strowl1': 1000 samples\n  Label 'tbsfin1': 1000 samples\n  Label 'thbeup1': 1000 samples\n  Label 'thlsch3': 1000 samples\n  Label 'trokin': 1000 samples\n  Label 'tropar': 1000 samples\n  Label 'trsowl': 1000 samples\n  Label 'turvul': 1000 samples\n  Label 'verfly': 1000 samples\n  Label 'watjac1': 1000 samples\n  Label 'wbwwre1': 1000 samples\n  Label 'whbant1': 1000 samples\n  Label 'whbman1': 1000 samples\n  Label 'whfant1': 1000 samples\n  Label 'whmtyr1': 1000 samples\n  Label 'whtdov': 1000 samples\n  Label 'whttro1': 1000 samples\n  Label 'whwswa1': 1000 samples\n  Label 'woosto': 1000 samples\n  Label 'y00678': 1000 samples\n  Label 'yebela1': 1000 samples\n  Label 'yebfly1': 1000 samples\n  Label 'yebsee1': 1000 samples\n  Label 'yecspi2': 1000 samples\n  Label 'yectyr1': 1000 samples\n  Label 'yehbla2': 1000 samples\n  Label 'yehcar1': 1000 samples\n  Label 'yelori1': 1000 samples\n  Label 'yeofly1': 1000 samples\n  Label 'yercac1': 1000 samples\n  Label 'ywcpar': 1000 samples\n\nDownsampling complete.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"model_yamnet = hub.load('https://tfhub.dev/google/yamnet/1')","metadata":{"_uuid":"47139e48-33e0-4a9d-897d-eec7987fad55","_cell_guid":"5d537e1d-6936-4c82-becb-f7edcbdfae58","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:21:42.380329Z","iopub.execute_input":"2025-05-04T10:21:42.380797Z","iopub.status.idle":"2025-05-04T10:21:48.593123Z","shell.execute_reply.started":"2025-05-04T10:21:42.380753Z","shell.execute_reply":"2025-05-04T10:21:48.591920Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"audio_embeddings = []\nfor i in tqdm(limited_audio_array, desc=\"Generating Audio Embeddings\"):\n    waveform = i / tf.int16.max\n    scores, embeddings, spectrogram = model_yamnet(waveform)\n    audio_embeddings.append(embeddings)","metadata":{"_uuid":"74922e51-60fc-4d4b-8d64-9282189624b9","_cell_guid":"072300bf-dab1-40f2-ba2d-90a0ef11906a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:21:48.594448Z","iopub.execute_input":"2025-05-04T10:21:48.594830Z","iopub.status.idle":"2025-05-04T10:58:46.968649Z","shell.execute_reply.started":"2025-05-04T10:21:48.594798Z","shell.execute_reply":"2025-05-04T10:58:46.967047Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating Audio Embeddings:   0%|          | 0/205604 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f94968f5cfd45f58b678ae848a47333"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"X = np.array(audio_embeddings) # Shape will be (N, 1024)\nX_reshaped = np.squeeze(X, axis=1) # Specify axis=1 to remove the middle dimension\nprint(f\"Generated embeddings array X with shape: {X_reshaped.shape}\")\n\n# Ensure you have the corresponding labels (length N)\ny = limited_labels_array","metadata":{"_uuid":"6978e048-de39-46ec-9e1b-2b8d0a94c65e","_cell_guid":"072fdaf9-4b36-4b9a-ba89-3e5ed7d926d3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:58:46.970004Z","iopub.execute_input":"2025-05-04T10:58:46.970337Z","iopub.status.idle":"2025-05-04T10:58:49.817062Z","shell.execute_reply.started":"2025-05-04T10:58:46.970306Z","shell.execute_reply":"2025-05-04T10:58:49.816060Z"}},"outputs":[{"name":"stdout","text":"Generated embeddings array X with shape: (205604, 1024)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# --- Label Encoding ---\nprint(\"\\nEncoding labels...\")\n# 1. Encode string/object labels to integers (0, 1, 2, ...)\nlabel_encoder = LabelEncoder()\ny_integer_encoded = label_encoder.fit_transform(y)\n\n# 2. Determine the number of unique classes\nnum_classes = len(label_encoder.classes_)\nprint(f\"Found {num_classes} unique classes: {label_encoder.classes_}\")\n\n# 3. Convert integer labels to one-hot encoding\ny_one_hot = tf.keras.utils.to_categorical(y_integer_encoded, num_classes=num_classes)\nprint(f\"One-hot encoded labels shape: {y_one_hot.shape}\") # Should be (N, num_classes)\n\n# --- Train/Test Split ---\nprint(\"\\nSplitting data into training and testing sets...\")\nxtrain, xtest, ytrain_one_hot, ytest_one_hot = train_test_split(\n    X_reshaped,                      # Your embeddings array (N, 1024)\n    y_one_hot,              # Your one-hot encoded labels (N, num_classes)\n    test_size=0.2,          # Fraction for the test set\n    random_state=42,        # For reproducibility\n    stratify=y_integer_encoded # IMPORTANT: Stratify based on original integer labels\n                               # to ensure class balance in train/test sets\n)\n\nprint(f\"xtrain shape: {xtrain.shape}\") # (N * 0.8, 1024)\nprint(f\"ytrain_one_hot shape: {ytrain_one_hot.shape}\") # (N * 0.8, num_classes)\nprint(f\"xtest shape: {xtest.shape}\") # (N * 0.2, 1024)\nprint(f\"ytest_one_hot shape: {ytest_one_hot.shape}\") # (N * 0.2, num_classes)","metadata":{"_uuid":"96703e48-f9a8-4045-8758-44dcf03a392f","_cell_guid":"da422fe0-2261-4a53-8774-3d2cd857e466","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:58:49.818167Z","iopub.execute_input":"2025-05-04T10:58:49.818464Z","iopub.status.idle":"2025-05-04T10:58:51.061282Z","shell.execute_reply.started":"2025-05-04T10:58:49.818436Z","shell.execute_reply":"2025-05-04T10:58:51.059970Z"}},"outputs":[{"name":"stdout","text":"\nEncoding labels...\nFound 206 unique classes: ['1139490' '1192948' '1194042' '126247' '1346504' '134933' '135045'\n '1462711' '1462737' '1564122' '21038' '21116' '21211' '22333' '22973'\n '22976' '24272' '24292' '24322' '41663' '41778' '41970' '42007' '42087'\n '42113' '46010' '47067' '476537' '476538' '48124' '50186' '517119'\n '523060' '528041' '52884' '548639' '555086' '555142' '566513' '64862'\n '65336' '65344' '65349' '65373' '65419' '65448' '65547' '65962' '66016'\n '66531' '66578' '66893' '67082' '67252' '714022' '715170' '787625'\n '81930' '868458' '963335' 'amakin1' 'amekes' 'ampkin1' 'anhing' 'babwar'\n 'bafibi1' 'banana' 'baymac' 'bbwduc' 'bicwre1' 'bkcdon' 'bkmtou1'\n 'blbgra1' 'blbwre1' 'blcant4' 'blchaw1' 'blcjay1' 'blctit1' 'blhpar1'\n 'blkvul' 'bobfly1' 'bobher1' 'brtpar1' 'bubcur1' 'bubwre1' 'bucmot3'\n 'bugtan' 'butsal1' 'cargra1' 'cattyr' 'chbant1' 'chfmac1' 'cinbec1'\n 'cocher1' 'cocwoo1' 'colara1' 'colcha1' 'compau' 'compot1' 'cotfly1'\n 'crbtan1' 'crcwoo1' 'crebob1' 'cregua1' 'creoro1' 'eardov1' 'fotfly'\n 'gohman1' 'grasal4' 'grbhaw1' 'greani1' 'greegr' 'greibi1' 'grekis'\n 'grepot1' 'gretin1' 'grnkin' 'grysee1' 'gybmar' 'gycwor1' 'labter1'\n 'laufal1' 'leagre' 'linwoo1' 'littin1' 'mastit1' 'neocor' 'norscr1'\n 'olipic1' 'orcpar' 'palhor2' 'paltan1' 'pavpig2' 'piepuf1' 'pirfly1'\n 'piwtyr1' 'plbwoo1' 'plctan1' 'plukit1' 'purgal2' 'ragmac1' 'rebbla1'\n 'recwoo1' 'rinkin1' 'roahaw' 'rosspo1' 'royfly1' 'rtlhum' 'rubsee1'\n 'rufmot1' 'rugdov' 'rumfly1' 'ruther1' 'rutjac1' 'rutpuf1' 'saffin'\n 'sahpar1' 'savhaw1' 'secfly1' 'shghum1' 'shtfly1' 'smbani' 'snoegr'\n 'sobtyr1' 'socfly1' 'solsan' 'soulap1' 'spbwoo1' 'speowl1' 'spepar1'\n 'srwswa1' 'stbwoo2' 'strcuc1' 'strfly1' 'strher' 'strowl1' 'tbsfin1'\n 'thbeup1' 'thlsch3' 'trokin' 'tropar' 'trsowl' 'turvul' 'verfly'\n 'watjac1' 'wbwwre1' 'whbant1' 'whbman1' 'whfant1' 'whmtyr1' 'whtdov'\n 'whttro1' 'whwswa1' 'woosto' 'y00678' 'yebela1' 'yebfly1' 'yebsee1'\n 'yecspi2' 'yectyr1' 'yehbla2' 'yehcar1' 'yelori1' 'yeofly1' 'yercac1'\n 'ywcpar']\nOne-hot encoded labels shape: (205604, 206)\n\nSplitting data into training and testing sets...\nxtrain shape: (164483, 1024)\nytrain_one_hot shape: (164483, 206)\nxtest shape: (41121, 1024)\nytest_one_hot shape: (41121, 206)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"\\nDefining and compiling the model...\")\n# --- Define Model ---\nmodel = models.Sequential([\n    # Input shape is now (1024,) after averaging embeddings\n    layers.Input(shape=(1024,)),\n    # Flatten layer is not needed if input is 1D before Dense\n    # layers.Flatten(), # Remove this\n    layers.Dense(200, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(200, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(200, activation='relu'),\n    # Output layer must have 'num_classes' units\n    layers.Dense(num_classes, activation='softmax')\n])\n\nmodel.summary()\n\n# --- Compile Model ---\n# Using categorical_crossentropy because ytrain is one-hot encoded\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nprint(\"Model compiled successfully.\")","metadata":{"_uuid":"c5c31e39-febd-40c2-ae5e-f3263d92a9d0","_cell_guid":"c0d42a1c-377d-4870-a195-8ab8fb3b7380","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:58:51.062638Z","iopub.execute_input":"2025-05-04T10:58:51.062984Z","iopub.status.idle":"2025-05-04T10:58:51.182009Z","shell.execute_reply.started":"2025-05-04T10:58:51.062955Z","shell.execute_reply":"2025-05-04T10:58:51.180769Z"}},"outputs":[{"name":"stdout","text":"\nDefining and compiling the model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │         \u001b[38;5;34m205,000\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m40,200\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m40,200\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m206\u001b[0m)                 │          \u001b[38;5;34m41,406\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">205,000</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">206</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,406</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m326,806\u001b[0m (1.25 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">326,806</span> (1.25 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m326,806\u001b[0m (1.25 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">326,806</span> (1.25 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Model compiled successfully.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',  # Metric to monitor (usually validation loss or accuracy)\n    patience=3,         # Number of epochs with no improvement after which training will be stopped\n    verbose=1,           # Set to 1 to print messages when stopping happens\n    mode='min',          # 'min' for loss/error metrics, 'max' for accuracy metrics\n    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity.\n)","metadata":{"_uuid":"c587b256-b5b9-4798-b931-57815075d174","_cell_guid":"626c0064-1aa5-4726-8e0a-2bef034c3163","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T10:58:51.183204Z","iopub.execute_input":"2025-05-04T10:58:51.183537Z","iopub.status.idle":"2025-05-04T10:58:51.192325Z","shell.execute_reply.started":"2025-05-04T10:58:51.183506Z","shell.execute_reply":"2025-05-04T10:58:51.190933Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# --- Train Model ---\nprint(\"\\nStarting model training...\")\nepochs = 100\nhistory = model.fit(\n    xtrain,\n    ytrain_one_hot,\n    epochs=epochs,\n    validation_split=0.1, # Optional: use part of training data for validation during training\n    callbacks=[early_stopping]\n    # Or use validation_data=(xtest, ytest_one_hot) - be careful not to \"tune\" on test set\n)\n\nprint(\"Model training finished.\")\n\n# --- Evaluate Model (Optional) ---\nprint(\"\\nEvaluating model on the test set...\")\nloss, accuracy = model.evaluate(xtest, ytest_one_hot, verbose=0)\nprint(f\"Test Loss: {loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"_uuid":"06142eb2-30f2-4552-9f83-bd1ba457ffe4","_cell_guid":"e17912b0-2a76-4801-ab44-24280d799315","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T11:25:57.894285Z","iopub.execute_input":"2025-05-04T11:25:57.895264Z","iopub.status.idle":"2025-05-04T11:34:19.821158Z","shell.execute_reply.started":"2025-05-04T11:25:57.895206Z","shell.execute_reply":"2025-05-04T11:34:19.819373Z"}},"outputs":[{"name":"stdout","text":"\nStarting model training...\nEpoch 1/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6ms/step - accuracy: 0.0650 - loss: 4.2522 - val_accuracy: 0.1635 - val_loss: 3.3676\nEpoch 2/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.1564 - loss: 3.3995 - val_accuracy: 0.2131 - val_loss: 3.0727\nEpoch 3/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.2019 - loss: 3.1329 - val_accuracy: 0.2608 - val_loss: 2.8494\nEpoch 4/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.2290 - loss: 2.9808 - val_accuracy: 0.2603 - val_loss: 2.8509\nEpoch 5/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.2509 - loss: 2.8720 - val_accuracy: 0.2973 - val_loss: 2.6915\nEpoch 6/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.2621 - loss: 2.8173 - val_accuracy: 0.2969 - val_loss: 2.6835\nEpoch 7/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.2745 - loss: 2.7688 - val_accuracy: 0.3032 - val_loss: 2.6446\nEpoch 8/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.2821 - loss: 2.7145 - val_accuracy: 0.3194 - val_loss: 2.5625\nEpoch 9/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.2891 - loss: 2.6879 - val_accuracy: 0.3245 - val_loss: 2.5428\nEpoch 10/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.2981 - loss: 2.6502 - val_accuracy: 0.3104 - val_loss: 2.6048\nEpoch 11/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.2957 - loss: 2.6687 - val_accuracy: 0.3259 - val_loss: 2.5616\nEpoch 12/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.3029 - loss: 2.6121 - val_accuracy: 0.3447 - val_loss: 2.4713\nEpoch 13/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.3065 - loss: 2.5995 - val_accuracy: 0.3352 - val_loss: 2.4743\nEpoch 14/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.3127 - loss: 2.5841 - val_accuracy: 0.3446 - val_loss: 2.4645\nEpoch 15/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.3056 - loss: 2.6406 - val_accuracy: 0.3594 - val_loss: 2.4242\nEpoch 16/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.3128 - loss: 2.5783 - val_accuracy: 0.3553 - val_loss: 2.4290\nEpoch 17/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.3146 - loss: 2.5704 - val_accuracy: 0.3428 - val_loss: 2.4771\nEpoch 18/100\n\u001b[1m4627/4627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.3126 - loss: 2.5845 - val_accuracy: 0.3446 - val_loss: 2.4843\nEpoch 18: early stopping\nRestoring model weights from the end of the best epoch: 15.\nModel training finished.\n\nEvaluating model on the test set...\nTest Loss: 2.4249\nTest Accuracy: 0.3530\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"_uuid":"de8f3436-a9a2-4c75-adaa-782966619c61","_cell_guid":"62e3b71b-89d1-4e54-86a9-550af561402b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}